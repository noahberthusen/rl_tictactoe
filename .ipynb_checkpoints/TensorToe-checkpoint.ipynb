{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic tic tac toe class\n",
    "class ttt:\n",
    "    def __init__(self):\n",
    "        self.board = np.array([None] * 9)\n",
    "\n",
    "    def update_board(self, player, move):\n",
    "        self.board[move] = player\n",
    "\n",
    "    @staticmethod\n",
    "    def check_for_win(board):\n",
    "        WIN_STATES = [\n",
    "            (0,1,2),\n",
    "            (3,4,5),\n",
    "            (6,7,8),\n",
    "            (0,3,6),\n",
    "            (1,4,7),\n",
    "            (2,5,8),\n",
    "            (0,4,8),\n",
    "            (2,4,6)\n",
    "        ]\n",
    "\n",
    "        for a, b, c in WIN_STATES:\n",
    "            if board[a] == board[b] == board[c] and board[a] == 1:\n",
    "                return 10\n",
    "            elif board[a] == board[b] == board[c] and board[a] == 2:\n",
    "                return -10\n",
    "        if len(ttt.legal_moves(board)) == 0:\n",
    "            return 5\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def legal_moves(board):\n",
    "        return np.asarray(np.where(board == None)).flatten()    \n",
    "\n",
    "    @staticmethod\n",
    "    def display_board(board):\n",
    "        def convert_board(board):\n",
    "            readable_board = [' '] * 9\n",
    "            for i in range(len(board)):\n",
    "                if board[i] == 1:\n",
    "                    readable_board[i] = 'X'\n",
    "                elif board[i] == 2:\n",
    "                    readable_board[i] = 'O'\n",
    "            return readable_board\n",
    "        board = convert_board(board)\n",
    "        print(' {:1} | {:1} | {:1}'.format(board[0],board[1],board[2]))\n",
    "        print('-----------')\n",
    "        print(' {:1} | {:1} | {:1}'.format(board[3],board[4],board[5]))\n",
    "        print('-----------')\n",
    "        print(' {:1} | {:1} | {:1}'.format(board[6],board[7],board[8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.inputs = None\n",
    "        self.output_q = None\n",
    "        self.target_q = None\n",
    "        self.train_step = None\n",
    "        self.session = None\n",
    "        self.probabilities = None\n",
    "        self.mse = None\n",
    "        self.build_network()\n",
    "        self.get_session()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.session.close()\n",
    "\n",
    "    def build_network(self):\n",
    "        with tf.variable_scope('q_network', reuse=tf.AUTO_REUSE):\n",
    "                self.inputs = tf.placeholder(tf.float32, shape=(None, 3, 3, 3))\n",
    "                self.target_q = tf.placeholder(tf.float32, [None, 9])\n",
    "\n",
    "                net = self.inputs\n",
    "                net = tf.layers.conv2d(inputs=net, filters=128, kernel_size=3,\n",
    "                                       kernel_regularizer=tf.contrib.layers.l1_l2_regularizer(),\n",
    "                                       data_format=\"channels_last\", padding='SAME', activation=tf.nn.relu)\n",
    "                net = tf.layers.conv2d(inputs=net, filters=128, kernel_size=3,\n",
    "                                       kernel_regularizer=tf.contrib.layers.l1_l2_regularizer(),\n",
    "                                       data_format=\"channels_last\", padding='SAME', activation=tf.nn.relu)\n",
    "                net = tf.layers.conv2d(inputs=net, filters=64, kernel_size=3,\n",
    "                                       kernel_regularizer=tf.contrib.layers.l1_l2_regularizer(),\n",
    "                                       data_format=\"channels_last\", padding='SAME', activation=tf.nn.relu)\n",
    "                net = tf.layers.flatten(net)\n",
    "                net = tf.layers.dense(net, 243, activation=tf.nn.relu,\n",
    "                                            kernel_initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "                self.output_q = tf.layers.dense(net, 9, activation=None,\n",
    "                                            kernel_initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "                self.probabilities = tf.nn.softmax(self.output_q)\n",
    "                self.mse = tf.losses.mean_squared_error(predictions=self.output_q, labels=self.target_q)\n",
    "                self.train_step = tf.train.GradientDescentOptimizer(learning_rate=0.005).minimize(self.mse)\n",
    "\n",
    "    def get_session(self):\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    def __init__(self):\n",
    "        self.nn = Network()\n",
    "        self.games = [ttt() for i in range(1000)]\n",
    "        self.board_position_log = []\n",
    "        self.action_log = []\n",
    "        self.next_max_log = []\n",
    "        self.values_log = []\n",
    "        self.rewards = []\n",
    "        self.epsilon = 1\n",
    "\n",
    "    @staticmethod\n",
    "    def nn_game_state(board):\n",
    "        nn_board = [0] * 27\n",
    "        for i in range(len(board)):\n",
    "            if (board[i] == 1):\n",
    "                nn_board[i] = 1\n",
    "            elif (board[i] == 2):\n",
    "                nn_board[i + 9] = 1\n",
    "            else:\n",
    "                nn_board[i + 18] = 1\n",
    "        nn_board = np.array(nn_board).reshape(3, 3, 3)\n",
    "        return nn_board\n",
    "\n",
    "    def new_game(self):\n",
    "        self.board_position_log = []\n",
    "        self.action_log = []\n",
    "        self.next_max_log = []\n",
    "        self.values_log = []\n",
    "\n",
    "    def calculate_targets(self):\n",
    "        targets = []\n",
    "        for i in range(len(self.values_log)):\n",
    "            target = np.copy(self.values_log[i])\n",
    "            target[self.action_log[i]] = 0.95 * self.next_max_log[i]\n",
    "            targets.append(target)\n",
    "        return targets\n",
    "\n",
    "        #Random play\n",
    "    def random_play(self):\n",
    "        def p1_turn(game):\n",
    "            def new_prob(prob, moves):\n",
    "                new_prob = [-1] * 9\n",
    "                for i in range(len(prob)):\n",
    "                    if i in moves:\n",
    "                        new_prob[i] = prob[i]\n",
    "                return new_prob\n",
    "            legal_moves = ttt.legal_moves(game.board)\n",
    "\n",
    "            self.board_position_log.append(game.board.copy())\n",
    "            nn_board = self.nn_game_state(game.board).reshape(1, 3, 3, 3)\n",
    "            q_values, probabilities = self.nn.session.run([self.nn.output_q, self.nn.probabilities], \n",
    "                                                        feed_dict={self.nn.inputs: nn_board})   \n",
    "            q_values = np.copy(q_values).flatten()\n",
    "            if (random.uniform(0, 1) > self.epsilon):\n",
    "                move = np.argmax(new_prob(np.array(probabilities).flatten(), legal_moves))\n",
    "            else:\n",
    "                move = np.random.choice(legal_moves)\n",
    "\n",
    "            if len(self.action_log) > 0:\n",
    "                self.next_max_log.append(q_values[move])\n",
    "            \n",
    "            self.action_log.append(move)\n",
    "            self.values_log.append(q_values)\n",
    "            game.update_board(1, move)\n",
    "            p2_turn(game)\n",
    "\n",
    "            # can add value to next_max_log here if you check to see if the game has been won, maybe\n",
    "            # maybe that was the issue with the other way: even for the winning states, I still had the max Q next thing\n",
    "            # but I don't think it mattered for the Q table ? I don't know\n",
    "\n",
    "            # I think the reward could be here as well. I'll just have to make sure the different arrays match regardless\n",
    "            # of the implementation. My way is more textbook\n",
    "\n",
    "            # also in this implementation, you have to train after every game, which isn't really experience replay\n",
    "\n",
    "            # if not(np.array_equal(old_board, game.board)):\n",
    "            #     updateQ(old_board, move, game.board, ttt.check_for_win(game.board))\n",
    "        \n",
    "        def p2_turn(game):\n",
    "            legal_moves = ttt.legal_moves(game.board)\n",
    "            if (len(legal_moves) != 0):\n",
    "                move = np.random.choice(legal_moves)\n",
    "                game.update_board(2, move)\n",
    "\n",
    "        # updating using NN\n",
    "        # def updateQ(s, a, new_s, r):\n",
    "        #     b = np.array([DeepQ.nn_game_state(s), DeepQ.nn_game_state(new_s)]).reshape(2,27)\n",
    "        #     q = sess.run(output_q, feed_dict={board: b})\n",
    "        #     # r + γ maxa′Q(s′)[a′]\n",
    "        #     q[0][a] = r + (0.9 * max(q[1]))\n",
    "        #     sess.run(train_step, feed_dict={board: np.array(DeepQ.nn_game_state(s)).reshape(1,27), target_q: np.array(q[0]).reshape(1,9)})\n",
    "        \n",
    "        for game in self.games:\n",
    "            self.new_game()\n",
    "\n",
    "            first = random.randint(1,2)\n",
    "            p_order = [first, 3-first]\n",
    "\n",
    "            if (p_order[0] == 2):\n",
    "                p2_turn(game)\n",
    "            while (ttt.check_for_win(game.board) == 0):\n",
    "                p1_turn(game)\n",
    "                    \n",
    "            # it makes sense to do it like this, though\n",
    "            # reward is usually 0 unless game is over. r + gamma max a'Q(s')[a']  --->  gamma max a'Q(s')[a']\n",
    "            # and when there is a reward the next state max should be 0 because there is no next state ? Maybe\n",
    "            # although in that situation there wouldn't be a penalty by gamma\n",
    "            self.next_max_log.append(ttt.check_for_win(game.board))\n",
    "            nn_input = [self.nn_game_state(x) for x in self.board_position_log]\n",
    "            targets = self.calculate_targets()\n",
    "\n",
    "            self.nn.session.run(self.nn.train_step, feed_dict={self.nn.inputs: nn_input, self.nn.target_q: targets})\n",
    "            self.rewards = np.append(self.rewards, ttt.check_for_win(game.board))\n",
    "\n",
    "            if (self.epsilon > 0.05):\n",
    "                self.epsilon *= 0.999991\n",
    "\n",
    "        print()\n",
    "        print(\"Player 1 Wins: \", np.asarray(np.where(self.rewards == 10)).flatten().size/len(self.rewards))\n",
    "        print(\"Player 2 Wins: \", np.asarray(np.where(self.rewards == -10)).flatten().size/len(self.rewards))\n",
    "        print(\"Ties: \", np.asarray(np.where(self.rewards == 5)).flatten().size/len(self.rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Player()\n",
    "p.random_play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Player.nn_game_state([None, None, None, None, None, None, None, None, None]).reshape(1, 3, 3, 3)\n",
    "b = Player.nn_game_state([1, None, 1, None, None, 2, None, None, 2]).reshape(1, 3, 3, 3)\n",
    "# b = np.array(b).reshape(1, 27)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.nn.session.run(p.nn.output_q, feed_dict={p.nn.inputs: b})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.nn.session.run(p.nn.probabilities, feed_dict={p.nn.inputs: b})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_play():\n",
    "    rewards = np.array([])\n",
    "    games = [ttt() for i in range(10000)]\n",
    "\n",
    "    def p_turn(player, game):\n",
    "        def new_prob(prob, moves):\n",
    "                new_prob = [-1] * 9\n",
    "                for i in range(len(prob)):\n",
    "                    if i in moves:\n",
    "                        new_prob[i] = prob[i]\n",
    "                return new_prob\n",
    "        \n",
    "        legal_moves = ttt.legal_moves(game.board)\n",
    "        nn_board = Player.nn_game_state(game.board).reshape(1, 3, 3, 3)\n",
    "        q_values, probabilities = p.nn.session.run([p.nn.output_q, p.nn.probabilities], \n",
    "                                                    feed_dict={p.nn.inputs: nn_board})   \n",
    "        q_values = np.copy(q_values).flatten()\n",
    "        if (player == 1):\n",
    "            move = np.argmax(new_prob(np.array(probabilities).flatten(), legal_moves))\n",
    "        else:\n",
    "            move = np.random.choice(legal_moves)\n",
    "        game.update_board(player, move)\n",
    "\n",
    "    for game in games:\n",
    "        first = random.randint(1,2)\n",
    "        p_order = [first, 3-first]\n",
    "\n",
    "        while (ttt.check_for_win(game.board) == 0):\n",
    "            p_turn(p_order[0], game)\n",
    "            if (ttt.check_for_win(game.board) != 0 or len(ttt.legal_moves(game.board)) == 0):\n",
    "                break\n",
    "            p_turn(p_order[1], game)\n",
    "    \n",
    "        rewards = np.append(rewards, ttt.check_for_win(game.board))\n",
    "\n",
    "    print()\n",
    "    print(\"Player 1 Wins: \", np.asarray(np.where(rewards == 10)).flatten().size/len(rewards)) # 0.9122, 0.9005, 0.904\n",
    "    print(\"Player 2 Wins: \", np.asarray(np.where(rewards == -10)).flatten().size/len(rewards)) # 0.0127, 0.0035, 0.0014\n",
    "    print(\"Ties: \", np.asarray(np.where(rewards == 5)).flatten().size/len(rewards)) # 0.071, 0.096, 0.094"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_play():\n",
    "    game = ttt()\n",
    "\n",
    "    def p_turn(game):\n",
    "        def new_prob(prob, moves):\n",
    "                new_prob = [-1] * 9\n",
    "                for i in range(len(prob)):\n",
    "                    if i in moves:\n",
    "                        new_prob[i] = prob[i]\n",
    "                return new_prob\n",
    "\n",
    "        legal_moves = ttt.legal_moves(game.board)\n",
    "        nn_board = Player.nn_game_state(game.board).reshape(1, 3, 3, 3)\n",
    "        q_values, probabilities = p.nn.session.run([p.nn.output_q, p.nn.probabilities], \n",
    "                                                    feed_dict={p.nn.inputs: nn_board})   \n",
    "        q_values = np.copy(q_values).flatten()\n",
    "        move = np.argmax(new_prob(np.array(probabilities).flatten(), legal_moves))\n",
    "\n",
    "        game.update_board(1, move)\n",
    "\n",
    "    def h_turn(game):\n",
    "        ttt.display_board(game.board)\n",
    "        move = input('Where would you like to play (0-8): ')\n",
    "        game.update_board(2, int(move))\n",
    "\n",
    "    first = random.randint(1,2)\n",
    "    p_order = [first, 3-first]\n",
    "    while (ttt.check_for_win(game.board) == 0):\n",
    "        if (p_order[0] == 1):\n",
    "            p_turn(game)\n",
    "        else:\n",
    "            h_turn(game)\n",
    "        if (ttt.check_for_win(game.board) != 0):\n",
    "            ttt.display_board(game.board)\n",
    "            break\n",
    "        if (p_order[0] == 1):\n",
    "            h_turn(game)\n",
    "        else:\n",
    "            p_turn(game)\n",
    "\n",
    "    print()\n",
    "    if (ttt.check_for_win(game.board) == 10):\n",
    "        print('AI wins')\n",
    "    elif (ttt.check_for_win(game.board) == -10):\n",
    "        print('You somehow beat the AI')\n",
    "    else:\n",
    "        print('You both played a perfect game. Tie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
